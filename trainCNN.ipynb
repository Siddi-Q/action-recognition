{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "\n",
    "import directoryFunctions\n",
    "import pathlib\n",
    "import config\n",
    "import data\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.optimizers   import Adam\n",
    "from tensorflow.keras.callbacks    import CSVLogger, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.layers       import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.losses       import CategoricalCrossentropy\n",
    "from tensorflow.keras.models       import load_model, Sequential\n",
    "\n",
    "\"\"\"\n",
    "Documentation:\n",
    "- numpy\n",
    "    1. array()\n",
    "        - https://numpy.org/doc/stable/reference/generated/numpy.array.html?highlight=array#numpy.array\n",
    "    2. ceil()\n",
    "        - https://numpy.org/doc/stable/reference/generated/numpy.ceil.html?highlight=ceil#numpy.ceil\n",
    "- os\n",
    "    1. path.sep\n",
    "- pathlib\n",
    "    1. Path(), /, glob()\n",
    "        - https://docs.python.org/3/library/pathlib.html\n",
    "- tensorflow\n",
    "    - data.Dataset\n",
    "        1. batch(), cache(), list_files(), map(), prefetch(), repeat(), shuffle()\n",
    "            - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/data/Dataset\n",
    "    - keras\n",
    "        - applications\n",
    "            1. InceptionV3()\n",
    "                - https://keras.io/api/applications/inceptionv3/\n",
    "        - callbacks\n",
    "            1. CSVLogger(), EarlyStopping(), ModelCheckpoint(), TensorBoard()\n",
    "                - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/callbacks\n",
    "        - layers\n",
    "            1. Dense()\n",
    "                - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/layers/Dense\n",
    "            2. GlobalAveragePooling2D()\n",
    "                - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/layers/GlobalAveragePooling2D\n",
    "        - losses\n",
    "            1. CategoricalCrossentropy()\n",
    "                - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/losses/CategoricalCrossentropy\n",
    "        - models\n",
    "            1. load_model()\n",
    "                - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/models/load_model\n",
    "            2. Sequential()\n",
    "                1. compile(), fit()\n",
    "                    - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/Sequential\n",
    "        - optimizers\n",
    "            1. Adam()\n",
    "                - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/optimizers/Adam\n",
    "    - image\n",
    "        1. convert_image_dtype(), decode_jpeg(), resize()\n",
    "            - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/image\n",
    "    -io\n",
    "        1. read_file()\n",
    "            - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/io/read_file\n",
    "    - strings\n",
    "        1. split()\n",
    "            - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/strings/split\n",
    "- time\n",
    "    1. time()\n",
    "        - https://docs.python.org/3/library/time.html#time.time\n",
    "\n",
    "Sources:\n",
    "    1. Input pipeline using tf.data\n",
    "        - https://www.tensorflow.org/tutorials/load_data/images\n",
    "    2. Transfer learning\n",
    "        - https://www.tensorflow.org/tutorials/images/transfer_learning\n",
    "    3. InceptionV3\n",
    "        - https://keras.io/api/applications/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function Name: getFrameCount\n",
    "Number of parameters: 1\n",
    "List of parameters:\n",
    "    1. dataDirectory | pathlib.Path | Path to a directory.\n",
    "Pre-condition:\n",
    "    1. 'dataDirectory' exists.\n",
    "Post-condition:\n",
    "    1. Returns the number of frames (.jpg files) in the folders within 'dataDirectory'. \n",
    "\"\"\"\n",
    "def getFrameCount(dataDirectory):\n",
    "    return len(list(dataDirectory.glob('*/*.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function Name: getDataSet\n",
    "Number of parameters: 1\n",
    "List of parameters:\n",
    "    1. dataDirectory | pathlib.Path | Path to a directory.\n",
    "Pre-condition:\n",
    "    1. 'dataDirectory' exists.\n",
    "Post-condition:\n",
    "    1. Returns a tf.data.Dataset object based on the files in the folders within 'dataDirectory'.\n",
    "\"\"\"\n",
    "def getDataSet(dataDirectory):\n",
    "    imagePathsDataset = tf.data.Dataset.list_files(str(dataDirectory/'*/*'), shuffle = True)\n",
    "    return imagePathsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function Name: getLabeledFrames\n",
    "Number of parameters: 1\n",
    "List of parameters:\n",
    "    1. imagePath | tf.Tensor (string) | Tensor of paths to images.\n",
    "Pre-condition:\n",
    "    1. Paths in 'imagePath' exists.\n",
    "Post-condition:\n",
    "    1. Returns a processed (decoded, converted, resized) image along with its label.\n",
    "\"\"\"\n",
    "def getLabeledFrames(imagePath):\n",
    "    def getLabel(imagePath):\n",
    "        # convert the path to a list of path components\n",
    "        parts = tf.strings.split(imagePath, os.path.sep)\n",
    "        # The second to last is the class-directory\n",
    "        return parts[-2] == classNames\n",
    "    \n",
    "    def decodeImg(img):\n",
    "        # convert the compressed string to a 3D uint8 tensor\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        # resize the image to the desired size.\n",
    "        return tf.image.resize(img, [299, 299])\n",
    "    \n",
    "    label = getLabel(imagePath)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(imagePath)\n",
    "    img = decodeImg(img)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function Name: prepareTrainDataset\n",
    "Number of parameters: 3\n",
    "List of parameters:\n",
    "    1. dataset           | tf.data.Dataset | Dataset object that contains the processed images and their labels.\n",
    "    2. cache             | bool/str        | If False or an empty string, cache is not used. \n",
    "                                             If True then dataset is cached in memory.\n",
    "                                             Otherwise, dataset is cached in a cache file.\n",
    "    3. shuffleBufferSize | int             | Size of the shuffle buffer.\n",
    "Pre-condition:\n",
    "    1. If 'cache' is a non-empty string, then it must be a directory that exists.\n",
    "Post-condition:\n",
    "    1. If specified, the dataset will be cached (either on memory or on disk).\n",
    "    2. Prefetches the next batched dataset.\n",
    "    3. Returns a shuffled and batched dataset.\n",
    "\"\"\"\n",
    "def prepareTrainDataset(dataset, cache, shuffleBufferSize):\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            dataset = dataset.cache(cache)\n",
    "        else:\n",
    "            dataset = dataset.cache()\n",
    "    \n",
    "    dataset = dataset.shuffle(buffer_size = shuffleBufferSize)\n",
    "    # Repeat forever\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function Name: getModel\n",
    "Number of parameters: 0\n",
    "List of parameters: n/a\n",
    "Pre-condition: n/a\n",
    "Post-condition:\n",
    "    1. Returns a compiled model (tf.keras.models.Sequential object).\n",
    "\"\"\"\n",
    "def getModel():\n",
    "    base_model = InceptionV3(input_shape = (299, 299, 3),\n",
    "                             include_top = False,\n",
    "                             weights     = 'imagenet')\n",
    "    base_model.trainable = False\n",
    "    global_average_layer = GlobalAveragePooling2D()\n",
    "    prediction_layer     = Dense(numClasses, activation = 'softmax')\n",
    "    \n",
    "    model = Sequential([base_model, global_average_layer, prediction_layer])\n",
    "    \n",
    "    model.compile(optimizer = Adam(lr = 0.0001),\n",
    "                  loss      = CategoricalCrossentropy(from_logits = True),\n",
    "                  metrics   = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function Name: fineTuneModel\n",
    "Number of parameters: 2\n",
    "List of parameters:\n",
    "    1. model      | tf.keras.models | Model to be fine-tuned.\n",
    "    2. fineTuneAt | int             | Layers indexed at and after this value are set to be trainable.\n",
    "Pre-condition: n/a\n",
    "Post-condition:\n",
    "    1. Sets some of the layers of the first layer in 'model' to be trainable.\n",
    "    2. Returns a compiled model (tf.keras.models.Sequential object).\n",
    "\"\"\"\n",
    "def fineTuneModel(model, fineTuneAt):\n",
    "    base_model = model.layers[0]\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    for layer in base_model.layers[:fineTuneAt]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model.compile(optimizer = Adam(lr = 0.00001),\n",
    "                  loss      = CategoricalCrossentropy(from_logits = True),\n",
    "                  metrics   = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function Name: trainModel\n",
    "Number of parameters: 8\n",
    "List of parameters:\n",
    "    1. model             | tf.keras.models | Model to be trained.\n",
    "    2. initial_epoch     | int             | Epoch to start training.\n",
    "    3. epochs            | int             | Epoch to stop training.\n",
    "    4. trainDataset      | tf.data.Dataset | Data used for training.\n",
    "    5. validationDataset | tf.data.Dataset | Data used for validation.\n",
    "    6. steps_per_epoch   | int             | Total number of steps in a 'training' epoch.\n",
    "    7. validation_steps  | int             | Total number of steps in a 'validation' epoch.\n",
    "    8. callbacks         | list            | List of callbacks. (tf.keras.callbacks.Callback)\n",
    "Pre-condition: n/a\n",
    "Post-condition:\n",
    "    1. Returns the model and History object.\n",
    "\"\"\"\n",
    "def trainModel(model, initial_epoch, epochs, trainDataset, validationDataset, steps_per_epoch, validation_steps, callbacks):\n",
    "    history = model.fit(trainDataset,\n",
    "                        initial_epoch    = initial_epoch, \n",
    "                        epochs           = epochs, \n",
    "                        validation_data  = validationDataset,\n",
    "                        steps_per_epoch  = steps_per_epoch,\n",
    "                        validation_steps = validation_steps,\n",
    "                        callbacks        = callbacks)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataObj    = data.Data()\n",
    "numClasses = dataObj.numClasses\n",
    "classNames = np.array(dataObj.classes)\n",
    "\n",
    "AUTOTUNE   = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function Name: main\n",
    "Number of parameters: 0\n",
    "List of parameters: n/a\n",
    "Pre-condition: n/a\n",
    "Post-condition:\n",
    "    1. Trains a model and saves callbacks to disk. \n",
    "    2. Nothing is returned.\n",
    "\"\"\"\n",
    "def main():\n",
    "    cf         = config.Config()\n",
    "    rootPath   = pathlib.Path(cf.rootPath)\n",
    "    framesPath = pathlib.Path(cf.framesPath)\n",
    "    \n",
    "    trainDataDirectory      = framesPath/'Train'\n",
    "    validationdataDirectory = framesPath/'Validation'\n",
    "    \n",
    "    cnnCallbacksDirectory    = rootPath/'Callbacks'/'CNN'/f'{numClasses}'\n",
    "    modelCheckpointDirectory = cnnCallbacksDirectory/'ModelCheckpoint'\n",
    "    tensorboardDirectory     = cnnCallbacksDirectory/'Tensorboard'\n",
    "    csvLoggerDirectory       = cnnCallbacksDirectory/'CSVLogger'\n",
    "    \n",
    "    directoryFunctions.createDirectory(modelCheckpointDirectory)\n",
    "    directoryFunctions.createDirectory(csvLoggerDirectory)\n",
    "    \n",
    "    trainDataset      = getDataSet(trainDataDirectory)\n",
    "    validationDataset = getDataSet(validationdataDirectory)\n",
    "    \n",
    "    trainDataset      = trainDataset.map(getLabeledFrames,      num_parallel_calls=AUTOTUNE)\n",
    "    validationDataset = validationDataset.map(getLabeledFrames, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    trainFrameCount      = getFrameCount(trainDataDirectory)\n",
    "    validationFrameCount = getFrameCount(validationdataDirectory)\n",
    "    \n",
    "    cachePath     = pathlib.Path(r\"./Cache\")\n",
    "    cacheFilePath = cachePath/'trainCNNDatasetCache'\n",
    "    \n",
    "    directoryFunctions.removeDirectory(cachePath)\n",
    "    directoryFunctions.createDirectory(cachePath)\n",
    "    \n",
    "    trainDataset      = prepareTrainDataset(trainDataset, str(cacheFilePath), int(trainFrameCount*(0.1)))\n",
    "    validationDataset = validationDataset.batch(BATCH_SIZE)\n",
    "    \n",
    "    steps_per_epoch  = np.ceil(trainFrameCount/BATCH_SIZE)\n",
    "    validation_steps = np.ceil(validationFrameCount/BATCH_SIZE)\n",
    "    \n",
    "    currTime = int(time.time())\n",
    "    modelCheckpointName = f'{currTime}' + '_CNN_{epoch:03d}_{val_loss:.2f}.h5'\n",
    "    modelCheckpoint = ModelCheckpoint(filepath       = str(modelCheckpointDirectory/modelCheckpointName),\n",
    "                                      save_best_only = True)\n",
    "    tensorboard     = TensorBoard(log_dir = str(tensorboardDirectory/f'{currTime}'))\n",
    "    csvLogger       = CSVLogger(str(csvLoggerDirectory/f'{currTime}.log'))\n",
    "    earlyStopping   = EarlyStopping(monitor = 'val_loss', patience = 5)\n",
    "    callbacks       = [modelCheckpoint, tensorboard, csvLogger, earlyStopping]\n",
    "    \n",
    "    initial_epoch    = 0\n",
    "    epochs           = 2\n",
    "    fine_tune_epochs = 10\n",
    "    fineTuneAt       = 249\n",
    "    \n",
    "    savedModelPath = \"\" # insert path to saved model (.h5 file) here\n",
    "    if savedModelPath == \"\":\n",
    "        model                  = getModel()\n",
    "        trained_model, history = trainModel(model, \n",
    "                                            initial_epoch, epochs, \n",
    "                                            trainDataset, validationDataset, \n",
    "                                            steps_per_epoch, validation_steps, \n",
    "                                            [])\n",
    "        fine_tuned_model       = fineTuneModel(trained_model, fineTuneAt)\n",
    "        initial_epoch          = history.epoch[-1]\n",
    "    else:\n",
    "        fine_tuned_model = load_model(savedModelPath)\n",
    "        initial_epoch    = 0\n",
    "        fine_tune_epochs = 1\n",
    "    \n",
    "    trained_model, history_fine = trainModel(fine_tuned_model, \n",
    "                                             initial_epoch, fine_tune_epochs, \n",
    "                                             trainDataset, validationDataset,\n",
    "                                             steps_per_epoch, validation_steps,\n",
    "                                             callbacks)\n",
    "    \n",
    "    directoryFunctions.removeDirectory(cachePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
