{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "\n",
    "import pathlib\n",
    "import data\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.applications        import InceptionV3\n",
    "from tensorflow.keras.optimizers          import Adam\n",
    "from tensorflow.keras.callbacks           import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.layers              import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.losses              import CategoricalCrossentropy\n",
    "from tensorflow.keras                     import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrameCount(dataDirectory):\n",
    "    return len(list(dataDirectory.glob('*/*.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet(dataDirectory):\n",
    "    imagePathsDataset = tf.data.Dataset.list_files(str(dataDirectory/'*/*'))\n",
    "    return imagePathsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabeledData(imagePath):\n",
    "    def getLabel(imagePath):\n",
    "        # convert the path to a list of path components\n",
    "        parts = tf.strings.split(imagePath, os.path.sep)\n",
    "        # The second to last is the class-directory\n",
    "        return parts[-2] == classNames\n",
    "    \n",
    "    def decodeImg(img):\n",
    "        # convert the compressed string to a 3D uint8 tensor\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        # resize the image to the desired size.\n",
    "        return tf.image.resize(img, [299, 299])\n",
    "    \n",
    "    label = getLabel(imagePath)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(imagePath)\n",
    "    img = decodeImg(img)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTrainDataset(dataset, cache, shuffleBufferSize):\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            dataset = dataset.cache(cache)\n",
    "        else:\n",
    "            dataset = dataset.cache()\n",
    "    \n",
    "    dataset = dataset.shuffle(buffer_size = shuffleBufferSize)\n",
    "    # Repeat forever\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    base_model = InceptionV3(input_shape = (299, 299, 3),\n",
    "                             include_top = False,\n",
    "                             weights     = 'imagenet')\n",
    "    base_model.trainable = False\n",
    "    global_average_layer = GlobalAveragePooling2D()\n",
    "    prediction_layer     = Dense(numClasses, activation = 'softmax')\n",
    "    \n",
    "    model = Sequential([base_model, global_average_layer, prediction_layer])\n",
    "    \n",
    "    model.compile(optimizer = Adam(lr = 0.0001),\n",
    "                  loss      = CategoricalCrossentropy(from_logits = True),\n",
    "                  metrics   = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fineTuneModel(model, fineTuneAt):\n",
    "    base_model = model.layers[0]\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    for layer in base_model.layers[:fineTuneAt]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model.compile(optimizer = Adam(lr = 0.00001),\n",
    "                  loss      = CategoricalCrossentropy(from_logits = True),\n",
    "                  metrics   = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, initial_epoch, epochs, trainDataset, validationDataset, steps_per_epoch, validation_steps, callbacks):\n",
    "    history = model.fit(trainDataset,\n",
    "                        initial_epoch    = initial_epoch, \n",
    "                        epochs           = epochs, \n",
    "                        validation_data  = validationDataset,\n",
    "                        steps_per_epoch  = steps_per_epoch,\n",
    "                        validation_steps = validation_steps,\n",
    "                        callbacks        = callbacks)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataObj    = data.Data()\n",
    "numClasses = dataObj.numClasses\n",
    "classNames = np.array(dataObj.classes)\n",
    "\n",
    "AUTOTUNE   = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    initial_epoch    = 0\n",
    "    epochs           = 1\n",
    "    fine_tune_epochs = 2\n",
    "    fineTuneAt       = 300\n",
    "    \n",
    "    cacheFilePath = \"./trainDatasetCache\"\n",
    "    rootPath      = pathlib.Path(r\"D:\\ActionRecognition\")\n",
    "    framesPath    = rootPath/'Frames'\n",
    "    \n",
    "    trainDataDirectory       = framesPath/'Train'\n",
    "    validationdataDirectory  = framesPath/'Validation'\n",
    "    cnnCallbacksDirectory    = rootPath/'Callbacks'/'CNN'/f'{numClasses}'\n",
    "    \n",
    "    trainFrameCount      = getFrameCount(trainDataDirectory)\n",
    "    validationFrameCount = getFrameCount(validationdataDirectory)\n",
    "    \n",
    "    trainDataset      = getDataSet(trainDataDirectory)\n",
    "    validationDataset = getDataSet(validationdataDirectory)\n",
    "    trainDataset      = trainDataset.map(getLabeledData,      num_parallel_calls=AUTOTUNE)\n",
    "    validationDataset = validationDataset.map(getLabeledData, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    trainDataset      = prepareTrainDataset(trainDataset, cacheFilePath, trainFrameCount)\n",
    "    validationDataset = validationDataset.batch(BATCH_SIZE)\n",
    "    \n",
    "    steps_per_epoch  = np.ceil(trainFrameCount/BATCH_SIZE)\n",
    "    validation_steps = np.ceil(validationFrameCount/BATCH_SIZE)\n",
    "    \n",
    "    modelCheckpointDirectory = cnnCallbacksDirectory/'ModelCheckpoint'\n",
    "    tensorboardDirectory     = cnnCallbacksDirectory/'Tensorboard'\n",
    "    \n",
    "    \n",
    "    modelCheckpoint = ModelCheckpoint(filepath       = str(modelCheckpointDirectory/'CNN_{epoch:03d}_{val_loss:.2f}'),\n",
    "                                      save_best_only = True)\n",
    "    tensorboard     = TensorBoard(log_dir = str(tensorboardDirectory/f'{int(time.time())}'))\n",
    "    \n",
    "    model                       = getModel()\n",
    "    trained_model, history      = trainModel(model, initial_epoch, epochs, \n",
    "                                             trainDataset, validationDataset,\n",
    "                                             steps_per_epoch, validation_steps,\n",
    "                                             [])\n",
    "    fine_tuned_model            = fineTuneModel(trained_model, fineTuneAt)\n",
    "\n",
    "    trained_model, history_fine = trainModel(fine_tuned_model, \n",
    "                                             history.epoch[-1], fine_tune_epochs, \n",
    "                                             trainDataset, validationDataset,\n",
    "                                             steps_per_epoch, validation_steps,\n",
    "                                             [modelCheckpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 15.0 steps, validate for 6.0 steps\n",
      "15/15 [==============================] - 82s 5s/step - loss: 1.0744 - accuracy: 0.4437 - val_loss: 1.0533 - val_accuracy: 0.5241\n",
      "Train for 15.0 steps, validate for 6.0 steps\n",
      "Epoch 1/2\n",
      "14/15 [===========================>..] - ETA: 4s - loss: 1.0554 - accuracy: 0.5156WARNING:tensorflow:From C:\\Users\\saddi\\Anaconda3\\envs\\Action_Recognition\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: D:\\ActionRecognition\\Callbacks\\CNN\\3\\ModelCheckpoint\\CNN_001_1.05\\assets\n",
      "15/15 [==============================] - 139s 9s/step - loss: 1.0545 - accuracy: 0.5229 - val_loss: 1.0487 - val_accuracy: 0.5241\n",
      "Epoch 2/2\n",
      "15/15 [==============================] - 80s 5s/step - loss: 1.0519 - accuracy: 0.5479 - val_loss: 1.0516 - val_accuracy: 0.5241\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
