{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "\n",
    "import directoryFunctions\n",
    "import pathlib\n",
    "import data\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.optimizers  import Adam\n",
    "from tensorflow.keras.callbacks   import CSVLogger, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.losses      import CategoricalCrossentropy\n",
    "from tensorflow.keras.layers      import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.models      import load_model, Sequential ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSequences(datasetType):\n",
    "    maxFrameCount = dataObj.getMaxFrameCount()\n",
    "    sequences = []\n",
    "    labels    = []\n",
    "    sequencesPath = pathlib.Path(r\"D:\\ActionRecognition\\Sequences\") ####\n",
    "    for dataRow in dataObj.data[:]:\n",
    "        if(datasetType == dataRow[0]):\n",
    "            sequencePath = sequencesPath/dataRow[0]/dataRow[1]/(dataRow[2] + \"_featureSequence.npy\") ####\n",
    "            sequence     = np.load(sequencePath)\n",
    "            sequence     = np.pad(sequence, ((0, maxFrameCount - int(dataRow[3])), (0, 0)), 'edge')\n",
    "            sequences.append(sequence)\n",
    "            label = dataObj.getClassIndex(dataRow[1])\n",
    "            labels.append(label)\n",
    "    return np.array(sequences), np.array(labels, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(sequences, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTrainDataset(dataset, cache, shuffleBufferSize):\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            dataset = dataset.cache(cache)\n",
    "        else:\n",
    "            dataset = dataset.cache()\n",
    "    \n",
    "    dataset = dataset.shuffle(buffer_size = shuffleBufferSize)\n",
    "    # Repeat forever\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(256, input_shape=(None, 2048)))\n",
    "    model.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    model.add(tf.keras.layers.Dense(numClasses, activation = 'softmax')) # change 3 to numClasses\n",
    "    \n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = 0.0001),\n",
    "                  loss      = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "                  metrics   = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, epochs, trainDataset, validationDataset, steps_per_epoch, validation_steps, callbacks):\n",
    "    history = model.fit(trainDataset, \n",
    "                        epochs = epochs,\n",
    "                        validation_data  = validationDataset,\n",
    "                        steps_per_epoch  = steps_per_epoch,\n",
    "                        validation_steps = validation_steps,\n",
    "                        callbacks = callbacks)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataObj    = data.Data()\n",
    "numClasses = dataObj.numClasses\n",
    "AUTOTUNE   = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    epochs = 5\n",
    "    \n",
    "    cachePath     = pathlib.Path(r\"./Cache\")\n",
    "    cacheFilePath = cachePath/'trainRNNDatasetCache'\n",
    "    directoryFunctions.removeDirectory(cachePath) ####\n",
    "    directoryFunctions.createDirectory(cachePath)\n",
    "    \n",
    "    rootPath = pathlib.Path(r\"D:\\ActionRecognition\")\n",
    "    rnnCallbacksDirectory = rootPath/'Callbacks'/'RNN'/f'{numClasses}'\n",
    "    \n",
    "    trainSeqCount      = dataObj.getDatasetCount('Train')\n",
    "    validationSeqCount = dataObj.getDatasetCount('Validation')\n",
    "    \n",
    "    trainSequences, trainLabels           = getSequences('Train')\n",
    "    validationSequences, validationLabels = getSequences('Validation')\n",
    "    \n",
    "    trainDataset      = getDataset(trainSequences, trainLabels)\n",
    "    validationDataset = getDataset(validationSequences, validationLabels)\n",
    "    \n",
    "    trainDataset      = prepareTrainDataset(trainDataset, str(cacheFilePath), trainSeqCount)\n",
    "    validationDataset = validationDataset.batch(BATCH_SIZE)\n",
    "    \n",
    "    steps_per_epoch  = np.ceil(trainSeqCount/BATCH_SIZE)\n",
    "    validation_steps = np.ceil(validationSeqCount/BATCH_SIZE)\n",
    "    \n",
    "    modelCheckpointDirectory = rnnCallbacksDirectory/'ModelCheckpoint'\n",
    "    tensorboardDirectory     = rnnCallbacksDirectory/'Tensorboard'\n",
    "    csvLoggerDirectory       = rnnCallbacksDirectory/'CSVLogger'\n",
    "    \n",
    "    directoryFunctions.createDirectory(csvLoggerDirectory)\n",
    "    directoryFunctions.createDirectory(modelCheckpointDirectory) ####\n",
    "    \n",
    "    modelCheckpoint = ModelCheckpoint(filepath       = str(modelCheckpointDirectory/(f'{int(time.time())}' + '_RNN_{epoch:03d}_{val_loss:.2f}.h5')),\n",
    "                                      save_best_only = True) #### add .h5 and added timeinfront of it\n",
    "    tensorboard     = TensorBoard(log_dir = str(tensorboardDirectory/f'{int(time.time())}'))\n",
    "    csvLogger       = CSVLogger(str(csvLoggerDirectory/f'{int(time.time())}.log'))\n",
    "    earlyStopping   = EarlyStopping(monitor = 'val_loss', patience = 5)\n",
    "    callbacks       = [modelCheckpoint, tensorboard, csvLogger, earlyStopping]\n",
    "    \n",
    "    ####---------------\n",
    "    savedModelPath = \"\" # insert path to saved model (.h5 file) here\n",
    "    if savedModelPath == \"\":\n",
    "        model = getModel()\n",
    "    else:\n",
    "        model  = load_model(savedModelPath)\n",
    "        epochs = 5\n",
    "    ####---------------\n",
    "    \n",
    "    trained_model, history = trainModel(model, epochs, \n",
    "                                        trainDataset, validationDataset, \n",
    "                                        steps_per_epoch, validation_steps, callbacks)\n",
    "    \n",
    "    directoryFunctions.removeDirectory(cachePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
