{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import pathlib\n",
    "import data\n",
    "\n",
    "from tensorflow.keras.optimizers  import Adam\n",
    "from tensorflow.keras.losses      import CategoricalCrossentropy\n",
    "from tensorflow.keras.layers      import Dense, Dropout, LSTM\n",
    "from tensorflow.keras             import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSequences(datasetType):\n",
    "    sequences = []\n",
    "    labels    = []\n",
    "    for dataRow in dataObj.data[:]:\n",
    "        if(datasetType == dataRow[0]):\n",
    "            sequencePath = pathlib.Path(r\"D:\\ActionRecognition\\Sequences\")/dataRow[0]/dataRow[1]/(dataRow[2] + \"_featureSequence.npy\")\n",
    "            sequence     = np.load(sequencePath)\n",
    "            sequences.append(sequence)\n",
    "            label = dataObj.getClassIndex(dataRow[1])\n",
    "            labels.append(label)\n",
    "    return np.array(sequences), np.array(labels, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(sequences, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTrainDataset(dataset, cache, shuffleBufferSize):\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            dataset = dataset.cache(cache)\n",
    "        else:\n",
    "            dataset = dataset.cache()\n",
    "    \n",
    "    dataset = dataset.shuffle(buffer_size = shuffleBufferSize)\n",
    "    # Repeat forever\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(256, input_shape=(None, 2048)))\n",
    "    model.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    model.add(tf.keras.layers.Dense(numClasses, activation = 'softmax')) # change 3 to numClasses\n",
    "    \n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = 0.0001),\n",
    "                  loss      = tf.keras.losses.CategoricalCrossentropy(from_logits = True),\n",
    "                  metrics   = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, epochs, trainDataset, validationDataset, steps_per_epoch, validation_steps, callbacks):\n",
    "    history = model.fit(trainDataset, \n",
    "                        epochs = epochs,\n",
    "                        validation_data  = validationDataset,\n",
    "                        steps_per_epoch  = steps_per_epoch,\n",
    "                        validation_steps = validation_steps,\n",
    "                        callbacks = callbacks)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataObj    = data.Data()\n",
    "numClasses = dataObj.numClasses\n",
    "AUTOTUNE   = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    epochs = 10\n",
    "    \n",
    "    cacheFilePath = \"./trainRNNDatasetCache\"\n",
    "    \n",
    "    trainSequences, trainLabels           = getSequences('Train')\n",
    "    validationSequences, validationLabels = getSequences('Validation')\n",
    "    \n",
    "    trainDataset      = getDataset(trainSequences, trainLabels)\n",
    "    validationDataset = getDataset(validationSequences, validationLabels)\n",
    "    \n",
    "    trainDataset      = prepareTrainDataset(trainDataset, cacheFilePath, 100)\n",
    "    validationDataset = validationDataset.batch(BATCH_SIZE)\n",
    "    \n",
    "    trainSeqCount      = dataObj.getDatasetCount('Train')\n",
    "    validationSeqCount = dataObj.getDatasetCount('Validation')\n",
    "    \n",
    "    steps_per_epoch  = np.ceil(trainSeqCount/BATCH_SIZE)\n",
    "    validation_steps = np.ceil(validationSeqCount/BATCH_SIZE)\n",
    "    \n",
    "    model = getModel()\n",
    "    trained_model, history = trainModel(model, epochs, trainDataset, validationDataset, steps_per_epoch, validation_steps, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
