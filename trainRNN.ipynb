{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "\n",
    "import directoryFunctions\n",
    "import pathlib\n",
    "import data\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.optimizers  import Adam\n",
    "from tensorflow.keras.callbacks   import CSVLogger, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.losses      import CategoricalCrossentropy\n",
    "from tensorflow.keras.layers      import Dense, Dropout, LSTM\n",
    "from tensorflow.keras             import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSequences(datasetType):\n",
    "    sequences = []\n",
    "    labels    = []\n",
    "    for dataRow in dataObj.data[:]:\n",
    "        if(datasetType == dataRow[0]):\n",
    "            sequencePath = pathlib.Path(r\"/home/jupyter/action-recognition/Sequences\")/dataRow[0]/dataRow[1]/(dataRow[2] + \"_featureSequence.npy\")\n",
    "            sequence     = np.load(sequencePath)\n",
    "            sequences.append(sequence)\n",
    "            label = dataObj.getClassIndex(dataRow[1])\n",
    "            labels.append(label)\n",
    "    return np.array(sequences), np.array(labels, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(sequences, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTrainDataset(dataset, cache, shuffleBufferSize):\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            dataset = dataset.cache(cache)\n",
    "        else:\n",
    "            dataset = dataset.cache()\n",
    "    \n",
    "    dataset = dataset.shuffle(buffer_size = shuffleBufferSize)\n",
    "    # Repeat forever\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(256, input_shape=(None, 2048)))\n",
    "    model.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    model.add(tf.keras.layers.Dense(numClasses, activation = 'softmax')) # change 3 to numClasses\n",
    "    \n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = 0.0001),\n",
    "                  loss      = tf.keras.losses.CategoricalCrossentropy(from_logits = True),\n",
    "                  metrics   = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, epochs, trainDataset, validationDataset, steps_per_epoch, validation_steps, callbacks):\n",
    "    history = model.fit(trainDataset, \n",
    "                        epochs = epochs,\n",
    "                        validation_data  = validationDataset,\n",
    "                        steps_per_epoch  = steps_per_epoch,\n",
    "                        validation_steps = validation_steps,\n",
    "                        callbacks = callbacks)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataObj    = data.Data()\n",
    "numClasses = dataObj.numClasses\n",
    "AUTOTUNE   = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    epochs = 5\n",
    "    \n",
    "    cachePath     = pathlib.Path(r\"./Cache\")\n",
    "    cacheFilePath = cachePath/'trainRNNDatasetCache'\n",
    "    directoryFunctions.createDirectory(cachePath)\n",
    "    \n",
    "    rootPath = pathlib.Path(r\"/home/jupyter/action-recognition/\")\n",
    "    rnnCallbacksDirectory = rootPath/'Callbacks'/'RNN'/f'{numClasses}'\n",
    "    \n",
    "    trainSeqCount      = dataObj.getDatasetCount('Train')\n",
    "    validationSeqCount = dataObj.getDatasetCount('Validation')\n",
    "    \n",
    "    trainSequences, trainLabels           = getSequences('Train')\n",
    "    validationSequences, validationLabels = getSequences('Validation')\n",
    "    \n",
    "    trainDataset      = getDataset(trainSequences, trainLabels)\n",
    "    validationDataset = getDataset(validationSequences, validationLabels)\n",
    "    \n",
    "    trainDataset      = prepareTrainDataset(trainDataset, str(cacheFilePath), trainSeqCount)\n",
    "    validationDataset = validationDataset.batch(BATCH_SIZE)\n",
    "    \n",
    "    steps_per_epoch  = np.ceil(trainSeqCount/BATCH_SIZE)\n",
    "    validation_steps = np.ceil(validationSeqCount/BATCH_SIZE)\n",
    "    \n",
    "    modelCheckpointDirectory = rnnCallbacksDirectory/'ModelCheckpoint'\n",
    "    tensorboardDirectory     = rnnCallbacksDirectory/'Tensorboard'\n",
    "    csvLoggerDirectory       = rnnCallbacksDirectory/'CSVLogger'\n",
    "    \n",
    "    directoryFunctions.createDirectory(csvLoggerDirectory)\n",
    "    \n",
    "    modelCheckpoint = ModelCheckpoint(filepath       = str(modelCheckpointDirectory/'RNN_{epoch:03d}_{val_loss:.2f}'),\n",
    "                                      save_best_only = True)\n",
    "    tensorboard     = TensorBoard(log_dir = str(tensorboardDirectory/f'{int(time.time())}'))\n",
    "    csvLogger       = CSVLogger(str(csvLoggerDirectory/f'{int(time.time())}.log'))\n",
    "    earlyStopping   = EarlyStopping(monitor = 'val_loss', patience = 5)\n",
    "    callbacks       = [modelCheckpoint, tensorboard, csvLogger, earlyStopping]\n",
    "    \n",
    "    model = getModel()\n",
    "    trained_model, history = trainModel(model, epochs, \n",
    "                                        trainDataset, validationDataset, \n",
    "                                        steps_per_epoch, validation_steps, callbacks)\n",
    "    \n",
    "    directoryFunctions.removeDirectory(cachePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 8.0 steps, validate for 3.0 steps\n",
      "Epoch 1/5\n",
      "6/8 [=====================>........] - ETA: 1s - loss: 3.2488 - accuracy: 0.2188WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /home/jupyter/action-recognition/Callbacks/RNN/3/ModelCheckpoint/RNN_001_3.74/assets\n",
      "8/8 [==============================] - 8s 952ms/step - loss: 3.2867 - accuracy: 0.2383 - val_loss: 3.7426 - val_accuracy: 0.5181\n",
      "Epoch 2/5\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 3.2083 - accuracy: 0.6339INFO:tensorflow:Assets written to: /home/jupyter/action-recognition/Callbacks/RNN/3/ModelCheckpoint/RNN_002_3.74/assets\n",
      "8/8 [==============================] - 4s 495ms/step - loss: 3.1549 - accuracy: 0.6133 - val_loss: 3.7424 - val_accuracy: 0.2410\n",
      "Epoch 3/5\n",
      "6/8 [=====================>........] - ETA: 0s - loss: 3.3477 - accuracy: 0.2708INFO:tensorflow:Assets written to: /home/jupyter/action-recognition/Callbacks/RNN/3/ModelCheckpoint/RNN_003_3.74/assets\n",
      "8/8 [==============================] - 4s 446ms/step - loss: 3.3090 - accuracy: 0.2656 - val_loss: 3.7423 - val_accuracy: 0.2289\n",
      "Epoch 4/5\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 3.3695 - accuracy: 0.4286INFO:tensorflow:Assets written to: /home/jupyter/action-recognition/Callbacks/RNN/3/ModelCheckpoint/RNN_004_3.74/assets\n",
      "8/8 [==============================] - 4s 496ms/step - loss: 3.3861 - accuracy: 0.4258 - val_loss: 3.7422 - val_accuracy: 0.3253\n",
      "Epoch 5/5\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 3.0752 - accuracy: 0.4107INFO:tensorflow:Assets written to: /home/jupyter/action-recognition/Callbacks/RNN/3/ModelCheckpoint/RNN_005_3.74/assets\n",
      "8/8 [==============================] - 4s 481ms/step - loss: 3.1028 - accuracy: 0.3945 - val_loss: 3.7422 - val_accuracy: 0.1446\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
