{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "\n",
    "import directoryFunctions\n",
    "import pathlib\n",
    "import config\n",
    "import random\n",
    "import data\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.optimizers  import Adam\n",
    "from tensorflow.keras.callbacks   import CSVLogger, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.losses      import CategoricalCrossentropy\n",
    "from tensorflow.keras.layers      import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.models      import load_model, Sequential\n",
    "\n",
    "\"\"\"\n",
    "Documentation:\n",
    "- numpy\n",
    "    1. array()\n",
    "        - https://numpy.org/doc/stable/reference/generated/numpy.array.html?highlight=array#numpy.array\n",
    "    2. ceil()\n",
    "        - https://numpy.org/doc/stable/reference/generated/numpy.ceil.html?highlight=ceil#numpy.ceil\n",
    "    3. load()\n",
    "        - https://numpy.org/doc/stable/reference/generated/numpy.load.html?highlight=load#numpy.load\n",
    "    4. pad()\n",
    "        - https://numpy.org/doc/stable/reference/generated/numpy.pad.html?highlight=pad#numpy.pad\n",
    "- pathlib\n",
    "    1. Path(), /, glob()\n",
    "        - https://docs.python.org/3/library/pathlib.html\n",
    "- random\n",
    "    1. shuffle()\n",
    "        - https://docs.python.org/3/library/random.html#random.shuffle\n",
    "- tensorflow\n",
    "    - data.Dataset\n",
    "        1. batch(), cache(), from_tensor_slices(), prefetch(), repeat(), shuffle()\n",
    "            - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/data/Dataset\n",
    "    - keras\n",
    "        - callbacks\n",
    "            1. CSVLogger(), EarlyStopping(), ModelCheckpoint(), TensorBoard()\n",
    "                - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/callbacks\n",
    "        - layers\n",
    "            1. Dense()\n",
    "                - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/layers/Dense\n",
    "            2. Dropout()\n",
    "                - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/layers/Dropout\n",
    "            3. LSTM()\n",
    "                - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/layers/LSTM\n",
    "        - losses\n",
    "            1. CategoricalCrossentropy()\n",
    "                - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/losses/CategoricalCrossentropy\n",
    "        - models\n",
    "            1. load_model()\n",
    "                - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/models/load_model\n",
    "            2. Sequential()\n",
    "                1. add(), compile(), fit()\n",
    "                    - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/Sequential\n",
    "        - optimizers\n",
    "            1. Adam()\n",
    "                - https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/optimizers/Adam\n",
    "- time\n",
    "    1. time()\n",
    "        - https://docs.python.org/3/library/time.html#time.time\n",
    "\n",
    "Sources:\n",
    "    1. Loading numpy arrays\n",
    "        - https://www.tensorflow.org/tutorials/load_data/numpy\n",
    "        - https://www.tensorflow.org/guide/data#consuming_numpy_arrays\n",
    "    2. Preparing data for training\n",
    "        - https://www.tensorflow.org/tutorials/load_data/images#basic_methods_for_training\n",
    "        - https://www.tensorflow.org/guide/data_performance\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function Name: getLabeledSequences\n",
    "Number of parameters: 1\n",
    "List of parameters:\n",
    "    1. datasetType | str | Determines whether the returned data (sequences) are from the Train, Validation or Test set.\n",
    "Pre-condition:\n",
    "    1. 'datasetType' must be either 'Train', 'Validation', or 'Test'. Otherwise empty arrays are returned.\n",
    "Post-condition:\n",
    "    1. Returns a numpy array of sequences (np.float32) and a numpy array of corresponding labels (np.uint8).\n",
    "\"\"\"\n",
    "def getLabeledSequences(datasetType):\n",
    "    maxFrameCount = dataObj.getMaxFrameCount()\n",
    "    sequences = []\n",
    "    labels    = []\n",
    "    dataRows = dataObj.data[:]\n",
    "    random.shuffle(dataRows)\n",
    "    sequencesPath = pathlib.Path(config.Config().sequencesPath)\n",
    "    for dataRow in dataRows:\n",
    "        if(datasetType == dataRow[0]):\n",
    "            sequencePath = sequencesPath/dataRow[0]/dataRow[1]/(dataRow[2] + \"_featureSequence.npy\")\n",
    "            sequence     = np.load(sequencePath)\n",
    "            sequence     = np.pad(sequence, ((0, maxFrameCount - int(dataRow[3])), (0, 0)), 'edge')\n",
    "            sequences.append(sequence)\n",
    "            label = dataObj.getClassIndex(dataRow[1])\n",
    "            labels.append(label)\n",
    "    return np.array(sequences), np.array(labels, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function Name: getDataset\n",
    "Number of parameters: 2\n",
    "List of parameters:\n",
    "    1. sequences | np.array | Numpy array of sequences.\n",
    "    2. labels    | np.array | Numpy array of labels.\n",
    "Pre-condition: n/a\n",
    "Post-condition:\n",
    "    1. Returns a tf.data.Dataset object based on the 'sequences' and 'labels' arrays. \n",
    "\"\"\"\n",
    "def getDataset(sequences, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function Name: prepareTrainDataset\n",
    "Number of parameters: 3\n",
    "List of parameters:\n",
    "    1. dataset           | tf.data.Dataset | Dataset object that contains the processed images and their labels.\n",
    "    2. cache             | bool/str        | If False or an empty string, cache is not used. \n",
    "                                             If True then dataset is cached in memory.\n",
    "                                             Otherwise, dataset is cached in a cache file.\n",
    "    3. shuffleBufferSize | int             | Size of the shuffle buffer.\n",
    "Pre-condition:\n",
    "    1. If 'cache' is a non-empty string, then it must be a directory that exists.\n",
    "Post-condition:\n",
    "    1. If specified, the dataset will be cached (either on memory or on disk).\n",
    "    2. Prefetches the next batched dataset.\n",
    "    3. Returns a shuffled and batched dataset.\n",
    "\"\"\"\n",
    "def prepareTrainDataset(dataset, cache, shuffleBufferSize):\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            dataset = dataset.cache(cache)\n",
    "        else:\n",
    "            dataset = dataset.cache()\n",
    "    \n",
    "    dataset = dataset.shuffle(buffer_size = shuffleBufferSize)\n",
    "    # Repeat forever\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function Name: getModel\n",
    "Number of parameters: 0\n",
    "List of parameters: n/a\n",
    "Pre-condition: n/a\n",
    "Post-condition:\n",
    "    1. Returns a compiled model (tf.keras.models.Sequential object).\n",
    "\"\"\"\n",
    "def getModel():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(4096, input_shape=(None, 2048), dropout=0.5))\n",
    "    model.add(tf.keras.layers.Dropout(0.15))\n",
    "    model.add(tf.keras.layers.Dense(512, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(numClasses, activation = 'softmax')) # change 3 to numClasses\n",
    "    \n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = 0.0000025),\n",
    "                  loss      = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "                  metrics   = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function Name: trainModel\n",
    "Number of parameters: 8\n",
    "List of parameters:\n",
    "    1. model             | tf.keras.models | Model to be trained.\n",
    "    2. epochs            | int             | Epoch to stop training.\n",
    "    3. trainDataset      | tf.data.Dataset | Data used for training.\n",
    "    4. validationDataset | tf.data.Dataset | Data used for validation.\n",
    "    5. steps_per_epoch   | int             | Total number of steps in a 'training' epoch.\n",
    "    6. validation_steps  | int             | Total number of steps in a 'validation' epoch.\n",
    "    7. callbacks         | list            | List of callbacks. (tf.keras.callbacks.Callback)\n",
    "Pre-condition: n/a\n",
    "Post-condition:\n",
    "    1. Returns the model and History object.\n",
    "\"\"\"\n",
    "def trainModel(model, epochs, trainDataset, validationDataset, steps_per_epoch, validation_steps, callbacks):\n",
    "    history = model.fit(trainDataset, \n",
    "                        epochs = epochs,\n",
    "                        validation_data  = validationDataset,\n",
    "                        steps_per_epoch  = steps_per_epoch,\n",
    "                        validation_steps = validation_steps,\n",
    "                        callbacks = callbacks)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataObj    = data.Data()\n",
    "numClasses = dataObj.numClasses\n",
    "AUTOTUNE   = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function Name: main\n",
    "Number of parameters: 0\n",
    "List of parameters: n/a\n",
    "Pre-condition: n/a\n",
    "Post-condition:\n",
    "    1. Trains a model and saves callbacks to disk. \n",
    "    2. Nothing is returned.\n",
    "\"\"\"\n",
    "def main():\n",
    "    cf         = config.Config()\n",
    "    rootPath   = pathlib.Path(cf.rootPath)\n",
    "    \n",
    "    rnnCallbacksDirectory    = rootPath/'Callbacks'/'RNN'/f'{numClasses}'\n",
    "    modelCheckpointDirectory = rnnCallbacksDirectory/'ModelCheckpoint'\n",
    "    tensorboardDirectory     = rnnCallbacksDirectory/'Tensorboard'\n",
    "    csvLoggerDirectory       = rnnCallbacksDirectory/'CSVLogger'\n",
    "    \n",
    "    directoryFunctions.createDirectory(csvLoggerDirectory)\n",
    "    directoryFunctions.createDirectory(modelCheckpointDirectory)\n",
    "    \n",
    "    trainSequences, trainLabels           = getLabeledSequences('Train')\n",
    "    validationSequences, validationLabels = getLabeledSequences('Validation')\n",
    "    \n",
    "    trainDataset      = getDataset(trainSequences, trainLabels)\n",
    "    validationDataset = getDataset(validationSequences, validationLabels)\n",
    "    \n",
    "    trainSeqCount      = dataObj.getDatasetCount('Train')\n",
    "    validationSeqCount = dataObj.getDatasetCount('Validation')\n",
    "    \n",
    "    cachePath     = pathlib.Path(r\"./Cache\")\n",
    "    cacheFilePath = cachePath/'trainRNNDatasetCache'\n",
    "    \n",
    "    directoryFunctions.removeDirectory(cachePath)\n",
    "    directoryFunctions.createDirectory(cachePath)\n",
    "    \n",
    "    trainDataset      = prepareTrainDataset(trainDataset, str(cacheFilePath), int(trainSeqCount*(0.1)))\n",
    "    validationDataset = validationDataset.batch(BATCH_SIZE)\n",
    "    \n",
    "    steps_per_epoch  = np.ceil(trainSeqCount/BATCH_SIZE)\n",
    "    validation_steps = np.ceil(validationSeqCount/BATCH_SIZE)\n",
    "    \n",
    "    currTime = int(time.time())\n",
    "    modelCheckpointName = f'{currTime}' + '_RNN_{epoch:03d}_{val_loss:.2f}.h5'\n",
    "    modelCheckpoint = ModelCheckpoint(filepath       = str(modelCheckpointDirectory/modelCheckpointName),\n",
    "                                      save_best_only = True)\n",
    "    tensorboard     = TensorBoard(log_dir = str(tensorboardDirectory/f'{currTime}'))\n",
    "    csvLogger       = CSVLogger(str(csvLoggerDirectory/f'{currTime}.log'))\n",
    "    earlyStopping   = EarlyStopping(monitor = 'val_loss', patience = 5)\n",
    "    callbacks       = [modelCheckpoint, tensorboard, csvLogger, earlyStopping]\n",
    "    \n",
    "    epochs = 50\n",
    "    \n",
    "    savedModelPath = \"\" # insert path to saved model (.h5 file) here\n",
    "    if savedModelPath == \"\":\n",
    "        model = getModel()\n",
    "    else:\n",
    "        model  = load_model(savedModelPath)\n",
    "        epochs = 5\n",
    "    \n",
    "    trained_model, history = trainModel(model, epochs, \n",
    "                                        trainDataset, validationDataset, \n",
    "                                        steps_per_epoch, validation_steps, callbacks)\n",
    "    \n",
    "    directoryFunctions.removeDirectory(cachePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
