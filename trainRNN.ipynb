{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "\n",
    "import directoryFunctions\n",
    "import pathlib\n",
    "import data\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.optimizers  import Adam\n",
    "from tensorflow.keras.callbacks   import CSVLogger, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.losses      import CategoricalCrossentropy\n",
    "from tensorflow.keras.layers      import Dense, Dropout, LSTM\n",
    "from tensorflow.keras             import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSequences(datasetType):\n",
    "    maxFrameCount = dataObj.getMaxFrameCount()\n",
    "    sequences = []\n",
    "    labels    = []\n",
    "    for dataRow in dataObj.data[:]:\n",
    "        if(datasetType == dataRow[0]):\n",
    "            sequencePath = pathlib.Path(r\"D:\\ActionRecognition\\Sequences\")/dataRow[0]/dataRow[1]/(dataRow[2] + \"_featureSequence.npy\")\n",
    "            sequence     = np.load(sequencePath)\n",
    "            sequence     = np.pad(sequence, ((0, maxFrameCount - int(dataRow[3])), (0, 0)), 'edge')\n",
    "            sequences.append(sequence)\n",
    "            label = dataObj.getClassIndex(dataRow[1])\n",
    "            labels.append(label)\n",
    "    return np.array(sequences), np.array(labels, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(sequences, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTrainDataset(dataset, cache, shuffleBufferSize):\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            dataset = dataset.cache(cache)\n",
    "        else:\n",
    "            dataset = dataset.cache()\n",
    "    \n",
    "    dataset = dataset.shuffle(buffer_size = shuffleBufferSize)\n",
    "    # Repeat forever\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(256, input_shape=(None, 2048)))\n",
    "    model.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    model.add(tf.keras.layers.Dense(numClasses, activation = 'softmax')) # change 3 to numClasses\n",
    "    \n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = 0.0001),\n",
    "                  loss      = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "                  metrics   = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, epochs, trainDataset, validationDataset, steps_per_epoch, validation_steps, callbacks):\n",
    "    history = model.fit(trainDataset, \n",
    "                        epochs = epochs,\n",
    "                        validation_data  = validationDataset,\n",
    "                        steps_per_epoch  = steps_per_epoch,\n",
    "                        validation_steps = validation_steps,\n",
    "                        callbacks = callbacks)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataObj    = data.Data()\n",
    "numClasses = dataObj.numClasses\n",
    "AUTOTUNE   = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    epochs = 5\n",
    "    \n",
    "    cachePath     = pathlib.Path(r\"./Cache\")\n",
    "    cacheFilePath = cachePath/'trainRNNDatasetCache'\n",
    "    directoryFunctions.createDirectory(cachePath)\n",
    "    \n",
    "    rootPath = pathlib.Path(r\"D:\\ActionRecognition\")\n",
    "    rnnCallbacksDirectory = rootPath/'Callbacks'/'RNN'/f'{numClasses}'\n",
    "    \n",
    "    trainSeqCount      = dataObj.getDatasetCount('Train')\n",
    "    validationSeqCount = dataObj.getDatasetCount('Validation')\n",
    "    \n",
    "    trainSequences, trainLabels           = getSequences('Train')\n",
    "    validationSequences, validationLabels = getSequences('Validation')\n",
    "    \n",
    "    trainDataset      = getDataset(trainSequences, trainLabels)\n",
    "    validationDataset = getDataset(validationSequences, validationLabels)\n",
    "    \n",
    "    trainDataset      = prepareTrainDataset(trainDataset, str(cacheFilePath), trainSeqCount)\n",
    "    validationDataset = validationDataset.batch(BATCH_SIZE)\n",
    "    \n",
    "    steps_per_epoch  = np.ceil(trainSeqCount/BATCH_SIZE)\n",
    "    validation_steps = np.ceil(validationSeqCount/BATCH_SIZE)\n",
    "    \n",
    "    modelCheckpointDirectory = rnnCallbacksDirectory/'ModelCheckpoint'\n",
    "    tensorboardDirectory     = rnnCallbacksDirectory/'Tensorboard'\n",
    "    csvLoggerDirectory       = rnnCallbacksDirectory/'CSVLogger'\n",
    "    \n",
    "    directoryFunctions.createDirectory(csvLoggerDirectory)\n",
    "    \n",
    "    modelCheckpoint = ModelCheckpoint(filepath       = str(modelCheckpointDirectory/'RNN_{epoch:03d}_{val_loss:.2f}'),\n",
    "                                      save_best_only = True)\n",
    "    tensorboard     = TensorBoard(log_dir = str(tensorboardDirectory/f'{int(time.time())}'))\n",
    "    csvLogger       = CSVLogger(str(csvLoggerDirectory/f'{int(time.time())}.log'))\n",
    "    earlyStopping   = EarlyStopping(monitor = 'val_loss', patience = 5)\n",
    "    callbacks       = [modelCheckpoint, tensorboard, csvLogger, earlyStopping]\n",
    "    \n",
    "    model = getModel()\n",
    "    trained_model, history = trainModel(model, epochs, \n",
    "                                        trainDataset, validationDataset, \n",
    "                                        steps_per_epoch, validation_steps, callbacks)\n",
    "    \n",
    "    directoryFunctions.removeDirectory(cachePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 8.0 steps, validate for 3.0 steps\n",
      "Epoch 1/5\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0246 - accuracy: 0.6205WARNING:tensorflow:From C:\\Users\\saddi\\Anaconda3\\envs\\Action_Recognition\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: D:\\ActionRecognition\\Callbacks\\RNN\\3\\ModelCheckpoint\\RNN_001_0.90\\assets\n",
      "8/8 [==============================] - 25s 3s/step - loss: 1.0106 - accuracy: 0.6484 - val_loss: 0.8998 - val_accuracy: 0.7470\n",
      "Epoch 2/5\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.8571 - accuracy: 0.8080INFO:tensorflow:Assets written to: D:\\ActionRecognition\\Callbacks\\RNN\\3\\ModelCheckpoint\\RNN_002_0.78\\assets\n",
      "8/8 [==============================] - 19s 2s/step - loss: 0.8504 - accuracy: 0.8203 - val_loss: 0.7822 - val_accuracy: 0.8795\n",
      "Epoch 3/5\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.7580 - accuracy: 0.9062INFO:tensorflow:Assets written to: D:\\ActionRecognition\\Callbacks\\RNN\\3\\ModelCheckpoint\\RNN_003_0.73\\assets\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.7515 - accuracy: 0.9023 - val_loss: 0.7320 - val_accuracy: 0.8795\n",
      "Epoch 4/5\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.6869 - accuracy: 0.9420INFO:tensorflow:Assets written to: D:\\ActionRecognition\\Callbacks\\RNN\\3\\ModelCheckpoint\\RNN_004_0.69\\assets\n",
      "8/8 [==============================] - 19s 2s/step - loss: 0.6856 - accuracy: 0.9453 - val_loss: 0.6911 - val_accuracy: 0.9036\n",
      "Epoch 5/5\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.6454 - accuracy: 0.9777INFO:tensorflow:Assets written to: D:\\ActionRecognition\\Callbacks\\RNN\\3\\ModelCheckpoint\\RNN_005_0.69\\assets\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.6416 - accuracy: 0.9805 - val_loss: 0.6860 - val_accuracy: 0.8795\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
