{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==2.0.0-alpha0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 332.1MB 73kB/s  eta 0:00:01 2% |▊                               | 7.3MB 33.0MB/s eta 0:00:10    6% |██                              | 20.7MB 35.6MB/s eta 0:00:09    10% |███▍                            | 35.7MB 22.0MB/s eta 0:00:14    45% |██████████████▌                 | 150.0MB 37.3MB/s eta 0:00:05    49% |████████████████                | 165.8MB 36.9MB/s eta 0:00:05\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0-alpha0) (1.11.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-gpu==2.0.0-alpha0)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting astor>=0.6.0 (from tensorflow-gpu==2.0.0-alpha0)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting numpy<2.0,>=1.14.5 (from tensorflow-gpu==2.0.0-alpha0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/27/e35e7c6e6a52fab9fcc64fc2b20c6b516eba930bb02b10ace3b38200d3ab/numpy-1.18.4-cp36-cp36m-manylinux1_x86_64.whl (20.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 20.2MB 2.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow-gpu==2.0.0-alpha0)\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
      "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0-alpha0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.0MB 15.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0-alpha0) (0.31.1)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow-gpu==2.0.0-alpha0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/7a/9744998129fce7e29c5f2d8b0f545913b7383e65d8366fc0ae98d11936af/grpcio-1.28.1.tar.gz (19.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 19.5MB 3.2MB/s eta 0:00:01                     | 286kB 45.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow-gpu==2.0.0-alpha0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 24.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0-alpha0) (3.6.1)\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow-gpu==2.0.0-alpha0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 25.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0 (from tensorflow-gpu==2.0.0-alpha0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 38.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0-alpha0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
      "\u001b[K    100% |████████████████████████████████| 419kB 27.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.2 (from tensorflow-gpu==2.0.0-alpha0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 29.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl (88kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 33.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.14.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (39.1.0)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.0.0)\n",
      "Building wheels for collected packages: termcolor, grpcio, absl-py\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for grpcio ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/00/4d/5f/07d0d4283911d2b917b867a11b1622d9d2cc8c286eefd10c33\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n",
      "Successfully built termcolor grpcio absl-py\n",
      "Installing collected packages: termcolor, astor, numpy, gast, grpcio, absl-py, markdown, tb-nightly, keras-preprocessing, keras-applications, tf-estimator-nightly, google-pasta, tensorflow-gpu\n",
      "  Found existing installation: numpy 1.14.3\n",
      "    Uninstalling numpy-1.14.3:\n",
      "      Successfully uninstalled numpy-1.14.3\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 gast-0.3.3 google-pasta-0.2.0 grpcio-1.28.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.2.2 numpy-1.18.4 tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 termcolor-1.1.0 tf-estimator-nightly-1.14.0.dev2019030115\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu==2.0.0-alpha0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import data\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications        import InceptionV3\n",
    "from tensorflow.keras.optimizers          import Adam\n",
    "from tensorflow.keras.callbacks           import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.layers              import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.losses              import CategoricalCrossentropy\n",
    "from tensorflow.keras                     import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-alpha0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageDataGenerators(train_data_directory, validation_data_directory):\n",
    "    train_image_generator      = ImageDataGenerator(rescale = 1./255)\n",
    "    validation_image_generator = ImageDataGenerator(rescale = 1./255)\n",
    "    \n",
    "    train_data_gen = train_image_generator.flow_from_directory(directory     = str(train_data_directory),\n",
    "                                                               target_size   = (299, 299),\n",
    "                                                               interpolation = \"lanczos\")\n",
    "    print(\"test4\")\n",
    "    validation_data_gen = validation_image_generator.flow_from_directory(directory    = str(validation_data_directory),\n",
    "                                                                        target_size   = (299, 299),\n",
    "                                                                        interpolation = \"lanczos\")\n",
    "    return train_data_gen, validation_data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5925 44\n",
      "17755 44\n",
      "BoxingPunchingBag 2\n",
      "Hammering 2\n",
      "CuttingInKitchen 2\n",
      "BlowDryHair 2\n",
      "HighJump 2\n",
      "BenchPress 2\n",
      "JavelinThrow 2\n",
      "Basketball 2\n",
      "HeadMassage 2\n",
      "FrontCrawl 2\n",
      "CleanAndJerk 2\n",
      "BreastStroke 2\n",
      "GolfSwing 2\n",
      "FloorGymnastics 2\n",
      "Billiards 2\n",
      "BrushingTeeth 2\n",
      "FrisbeeCatch 2\n",
      "IceDancing 2\n",
      "Haircut 2\n",
      "CricketShot 2\n",
      "HandStandPushups 2\n",
      "BandMarching 2\n",
      "Diving 2\n",
      "HammerThrow 2\n",
      "HulaHoop 2\n",
      "BalanceBeam 2\n",
      "HorseRiding 2\n",
      "BaseballPitch 2\n",
      "ApplyLipstick 2\n",
      "BabyCrawling 2\n",
      "Fencing 2\n",
      "BlowingCandles 2\n",
      "ApplyEyeMakeup 2\n",
      "CliffDiving 2\n",
      "CricketBowling 2\n",
      "BasketballDunk 2\n",
      "Archery 2\n",
      "FieldHockeyPenalty 2\n",
      "Biking 2\n",
      "BodyWeightSquats 2\n",
      "BoxingSpeedBag 2\n",
      "Drumming 2\n",
      "HorseRace 2\n",
      "Bowling 2\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "dict_ = {}\n",
    "\n",
    "files = folders = 0\n",
    "path = pathlib.Path(r\"/home/ec2-user/SageMaker/action-recognition/Frames/Test\")\n",
    "for _, dirnames, filenames in os.walk(path):\n",
    "      # ^ this idiom means \"we won't be using this value\"\n",
    "        files += len(filenames)\n",
    "        folders += len(dirnames)\n",
    "        for dirname in dirnames: \n",
    "            dict_[dirname] = 1\n",
    "        \n",
    "\n",
    "print (files, folders)\n",
    "\n",
    "files = folders = 0\n",
    "path = pathlib.Path(r\"/home/ec2-user/SageMaker/action-recognition/Frames/Train\")\n",
    "for _, dirnames, filenames in os.walk(path):\n",
    "      # ^ this idiom means \"we won't be using this value\"\n",
    "        files += len(filenames)\n",
    "        folders += len(dirnames)\n",
    "        for dirname in dirnames :\n",
    "            if dirname in dict_:\n",
    "                dict_[dirname] += 1\n",
    "\n",
    "print (files, folders)\n",
    "\n",
    "for key in dict_ :\n",
    "    print(key, dict_[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(numClasses):\n",
    "    base_model = InceptionV3(input_shape = (299, 299, 3),\n",
    "                             include_top = False,\n",
    "                             weights     = 'imagenet')\n",
    "    base_model.trainable = False\n",
    "    global_average_layer = GlobalAveragePooling2D()\n",
    "    prediction_layer     = Dense(numClasses, activation = 'softmax')\n",
    "    \n",
    "    model = Sequential([base_model, global_average_layer, prediction_layer])\n",
    "    \n",
    "    model.compile(optimizer = Adam(lr = 0.0001),\n",
    "                  loss      = CategoricalCrossentropy(from_logits = True),\n",
    "                  metrics   = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fineTuneModel(model, fineTuneAt):\n",
    "    base_model = model.layers[0]\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    for layer in base_model.layers[:fineTuneAt]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model.compile(optimizer = Adam(lr = 0.00001),\n",
    "                  loss      = CategoricalCrossentropy(from_logits = True),\n",
    "                  metrics   = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, initial_epoch, epochs, train_data_gen, validation_data_gen, callbacks):\n",
    "    history = model.fit(train_data_gen,\n",
    "                        initial_epoch   = initial_epoch, \n",
    "                        epochs          = epochs, \n",
    "                        validation_data = validation_data_gen,\n",
    "                        callbacks       = callbacks)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataObj    = data.Data()\n",
    "    numClasses = dataObj.numClasses\n",
    "    \n",
    "    initial_epoch    = 0\n",
    "    epochs           = 5\n",
    "    fine_tune_epochs = 9\n",
    "    fineTuneAt       = 300\n",
    "    \n",
    "    train_data_directory      = pathlib.Path(r\"/home/ec2-user/SageMaker/action-recognition/Frames/Train\")\n",
    "    validation_data_directory = pathlib.Path(r\"/home/ec2-user/SageMaker/action-recognition/Frames/Test\")\n",
    "    train_data_gen, validation_data_gen = getImageDataGenerators(train_data_directory, validation_data_directory)\n",
    "    \n",
    "    modelCheckpointDirectory = pathlib.Path(r\"/home/ec2-user/SageMaker/action-recognition/Callbacks/CNN/ModelCheckpoint\")\n",
    "    tensorboardDirectory     = pathlib.Path(r\"/home/ec2-user/SageMaker/action-recognition/Callbacks/CNN/Tensorboard\")\n",
    "    \n",
    "    modelCheckpoint = ModelCheckpoint(filepath       = str(modelCheckpointDirectory/'CNN_{epoch:03d}_{val_loss:.2f}'),\n",
    "                                      save_best_only = True)\n",
    "    tensorboard     = TensorBoard(log_dir = str(tensorboardDirectory/f'{int(time.time())}'))\n",
    "    \n",
    "    model                       = getModel(numClasses)\n",
    "    trained_model, history      = trainModel(model, initial_epoch, epochs, train_data_gen, validation_data_gen, [])\n",
    "    fine_tuned_model            = fineTuneModel(trained_model, fineTuneAt)\n",
    "    trained_model, history_fine = trainModel(fine_tuned_model, \n",
    "                                             history.epoch[-1], fine_tune_epochs, \n",
    "                                             train_data_gen, validation_data_gen, \n",
    "                                             [modelCheckpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "1\n",
      "Found 17755 images belonging to 44 classes.\n",
      "test4\n",
      "Found 5925 images belonging to 44 classes.\n",
      "2\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (32, 44) was passed for an output of shape (None, 45) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-35f5a6b6e710>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m                       \u001b[0;34m=\u001b[0m \u001b[0mgetModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumClasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mfine_tuned_model\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0mfineTuneModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfineTuneAt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     trained_model, history_fine = trainModel(fine_tuned_model, \n",
      "\u001b[0;32m<ipython-input-15-fb72a758fde5>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(model, initial_epoch, epochs, train_data_gen, validation_data_gen, callbacks)\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mepochs\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_data_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                         callbacks       = callbacks)\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    768\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    771\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager_dataset_or_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m       \u001b[0;31m# Make sure that y, sample_weights, validation_split are not passed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1236\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[1;32m   1237\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m         extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2655\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2657\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2658\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    510\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    511\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (32, 44) was passed for an output of shape (None, 45) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
